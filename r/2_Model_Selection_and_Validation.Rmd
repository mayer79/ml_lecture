---
title: "Model Selection and Validation"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    theme: united
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

In the previous chapter, we have met performance measures like the RMSE or the deviance to measure how good our models are. Unfortunately, we cannot fully rely on these values due to overfitting: The more our models overfit, the less we can trust in their "insample" performance, i.e. the performance on the data used to calculate the models. Selecting models based on their insample performance is equally bad. Overfitting should not be rewarded!

In this chapter, we will meet ways to estimate the performance of a model in a fair way and use it to select the best model among alternatives. They are all based on data splitting techniques, where the models are evaluated on fresh data not used for model calculation. Before introducing these techniques, we will meet a competitor of the linear model.

# Nearest-Neighbour

A very simple and intuitive alternative to the linear model is the k-nearest-neighbour approach, originally introduced by Evelyn Fix and J. L. Hodges in an unpublished technical report in 1951. It can be applied for both regression and classification and works without fitting anything. The prediction for an observation is obtained by 

1. searching the closest k neighbours in the data set and then 
2. combining their responses. 

By "nearest" we usually mean Euclidean distance in the covariable space. If covariables are not on the same scale, it makes sense to *standardize* them first by subtracting the mean and dividing by the standard deviation. Otherwise, distances would be dominated by the covariable on the largest scale. Categorical features need to be one-hot- or integer-encoded first. Note that one-hot-encoded covariables may or may not be standardized.

For regression tasks, the responses of the k nearest neighbours are often combined by computing their arithmetic mean. For classification tasks, they are condensed by their most frequent value or to class probabilities.

## Example: nearest-neighbour

What prediction would we get with 5-nearest-neighbour regression for the 10'000th row of the diamonds data set?

```{r}
library(ggplot2)
library(FNN)

# Covariable matrix
x <- c("carat", "color", "cut", "clarity")
X <- scale(data.matrix(diamonds[, x]))

# The 10'000th observation
diamonds[10000, c("price", x)]

# Its prediction
knn.reg(X, test = X[10000, ], k = 5, y = diamonds$price)

# Its five nearest neighbours
neighbours <- c(knnx.index(X, X[10000, , drop = FALSE], k = 5))
diamonds[neighbours, ]
```

**Comments** 

- The five nearest diamonds are extremely similar. The second one is the observation of interest itself, introducing a relevant amount of overfitting.
- The average price of these five observations gives us the nearest-neighbour prediction for the 10'000th diamond.
- Would we get better results for a different choice of the number of neighbours k?
- Three lines are identical up to the perspective variables (`table`, `x`, `y`, `z`). These rows most certainly represent the same diamond, introducing additional overfit. We need to keep this problematic aspect of the diamonds data in mind.

**Motivation for this chapter:** Insample, a 1-nearest-neighbour regression predicts without error, a consequence of massive overfitting. This hypothetical example indicates that insample performance is often not worth a penny. Models need to be evaluated on fresh, independent data not used for model calculation. This leads us to *simple validation*.

# Simple Validation

With simple validation, the original data set is partitioned into *training* data used to calculate the models and a separate *validation* data set used to evaluate model performance and/or to select models. Typically, 10%-30% of rows are used for validation.

We can use the validation performance to compare *algorithms* (e.g. regression versus k-nearest-neighbour) and also to choose their *hyperparameters* like the "k" of k-nearest-neighbour.

Furthermore, the performance difference between training and validation data indicates the amount of overfitting.

## Example: simple validation

What k provides the best RMSE on 20% validation data of the diamonds data?

```{r}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 80% for "training" and 20% for validation
ix <- partition(diamonds$price, p = c(train = 0.8, valid = 0.2), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
valid <- diamonds[ix$valid, ]

y_train <- train$price
y_valid <- valid$price

# Standardize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to validation data
X_valid <- scale(data.matrix(valid[, x]),
                 center = attr(X_train, "scaled:center"),
                 scale = attr(X_train, "scaled:scale"))

# Tuning grid with different values for parameter k
paramGrid <- data.frame(train = NA, valid = NA, k = 1:20)
    
# Calculate performance for each row in the parameter grid
for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  
  # Performance on training data
  pred_train <- knn.reg(X_train, test = X_train, k = k, y = y_train)$pred
  paramGrid[i, "train"] <- rmse(y_train, pred_train)
  
  # Performance on valid data
  pred_valid <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
  paramGrid[i, "valid"] <- rmse(y_valid, pred_valid)
}

# Plot results
pivot_longer(paramGrid, cols = -k, values_to = "RMSE", names_to = "Data") %>% 
ggplot(aes(x = k, y = RMSE, group = Data, color = Data)) +
  geom_point() +
  geom_line()
```

**Comments**

- The amount of overfitting decreases for growing k, which makes sense.
- Selecting k based on the training data would lead to a suboptimal model.
- Based on the validation data, we would choose $k=4$. It has a minimal RMSE of 602 USD.
- Why is the RMSE on the training data not 0 for 1-nearest-neighbour?
- Why is it problematic that some diamonds appear multiple times in the dataset?

# Cross-Validation (CV)

If our data set is large and training takes long, then the simple validation strategy introduced above is usually good enough. For smaller data or if training is fast, there is a better alternative that utilizes the data in a more economic way and takes more robust decisions. It is called **k-fold cross-validation** and works as follows:

1. Split the data into k pieces $D = \{D_1, \dots, D_k\}$ called "folds". Typical values for k are five or ten.
2. Set aside one of the pieces ($D_j$) for validation.
3. Fit the model on all other pieces, i.e. on $D \setminus D_j$.
4. Calculate the model performance on the validation data $D_j$.
5. Repeat Steps 2-4 until each piece was used for validation once.
6. The average of the k model performances yields the *CV performance* of the model.

The CV performance is a good basis to choose the best and final model among alternatives. The final model is retrained on all folds.

Note: If cross-validation is fast, you can repeat the process for additional data splits. Such *repeated* cross-validation leads to even more robust results.

## Example: cross-validation

We now use five-fold CV on the diamonds data to find the optimal k, i.e. to *tune* our nearest-neighbour approach.

```{r}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Response and scaled covariable matrix
y <- diamonds$price
X <- diamonds %>% 
  select(carat, color, cut, clarity) %>% 
  data.matrix() %>% 
  scale()

# Split diamonds into folds
nfolds <- 5
folds <- create_folds(y, k = nfolds, seed = 9838, type = "basic")

# Tuning grid with different values for parameter k
paramGrid <- data.frame(RMSE = NA, k = 1:20)
    
# Calculate performance for each row in the parameter grid
for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  
  scores <- c()
  
  for (fold in folds) {
    y_train <- y[fold]
    y_valid <- y[-fold]
    
    X_train <- X[fold, ]
    X_valid <- X[-fold, ]

    pred <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
    scores[length(scores) + 1] <- rmse(y_valid, pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}
 
paramGrid

ggplot(paramGrid, aes(x = k, y = RMSE)) +
  geom_point() +
  geom_line() +
  ggtitle("Performance by cross-validation")
```

**Comment:** Using 5 neighbours seems to be the best choice with a CV RMSE of 619 USD. Again, the fact that certain diamonds appear multiple times leaves a slightly bad feeling. Should we really trust these results?

# Grid Search

In the above example, we have systematically compared the CV-performance of k-nearest-neighbour by iterating over a grid of possible values for k. Such strategy to *tune* models resp. to select hyperparameters of a model is called **grid search CV**. In the next chapter, we will meet situations where multiple parameters have to be optimized simultaneously. Then, the number of parameter combinations and the grid size explode. To save time, we could evaluate only a random subset of parameter combinations, an approach called **randomized search CV**.

# Test Data and Final Workflow

Often, modeling involves many decisions. Even if guided by (cross-)validation, each decision tends to make the resulting final model look better than it is, an effect that can be called *overfitting on the validation data*. As a consequence, we often do not know how well the final model will perform in reality. As a solution, we can set aside a small *test* data set used to assess the performance of the *final* model. A size of 5%-20% is usually sufficient. 
It is important to look at the test data just once at the very end of the modeling process - after each decision has been made.

Note: Such additional test data set is only necessary if one uses the validation data set to *make decisions*. If the validation data set is just used to assess the true performance of a model, then we do not need this extra data set. Then, we can use the terms "validation data" and "test data" interchangeably.

Depending on whether one does simple validation or cross-validation, the typical workflow is as follows:

**Workflow A**

1. Split data into train/valid/test, e.g. by ratios 70%/20%/10%.
2. Train different models on the training data and assess their performance on the validation data. Choose the best model, retrain it on the combination of training and validation data and call it "final model".
3. Assess the performance of the final model on the test data.

**Workflow B**

1. Split data into train/test, e.g. by ratios 90%/10%.
2. Evaluate and tune different models by k-fold cross-validation on the training data. Choose the best model, retrain it on the full training data.
3. Assess performance of the final model on the test data.

The only difference across the two workflows is whether to use simple validation or cross-validation for making decisions.

## Example: test data

We will now go through Workflow B for our diamond price model. We will (1) tune the "k" of our nearest-neighbour regression and (2) compete with a linear regression.

```{r}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

dia <- diamonds[, c("price", x)]
# dia <- unique(diamonds)  #  -> Exercise 2

# Split diamonds into 90% for training and 10% for testing
ix <- partition(dia$price, p = c(train = 0.9, test = 0.1), seed = 9838, type = "basic")

train <- dia[ix$train, ]
test <- dia[ix$test, ]

y_train <- train$price
y_test <- test$price

# Standardize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to test data
X_test <- scale(data.matrix(test[, x]),
                center = attr(X_train, "scaled:center"),
                scale = attr(X_train, "scaled:scale"))

# Split training data into folds
nfolds <- 5
folds <- create_folds(y_train, k = nfolds, seed = 9838, type = "basic")

# Cross-validation performance of k-nearest-neighbour for k = 1-20
paramGrid <- data.frame(RMSE = NA, k = 1:20)

for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  scores <- c()
  
  for (fold in folds) {
    pred <- knn.reg(X_train[fold, ], test = X_train[-fold, ], 
                    k = k, y = y_train[fold])$pred
    scores[length(scores) + 1] <- rmse(y_train[-fold], pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}

# Best k along with its CV performance
paramGrid[order(paramGrid$RMSE)[1], ]

# Cross-validation performance of linear regression
rmse_reg <- c()

for (fold in folds) {
  fit <- lm(price ~ log(carat) + color + cut + carat, data = train[fold, ])
  pred <- predict(fit, newdata = train[-fold, ])
  rmse_reg[length(rmse_reg) + 1] <- rmse(y_train[-fold], pred)
}
(rmse_reg <- mean(rmse_reg))

# The overall best model is 6-nearest-neighbour
pred <- knn.reg(X_train, test = X_test, k = 6, y = y_train)$pred

# Test performance for the best model
rmse(y_test, pred)
```

**Comments** 

- 6-nearest-neighbour regression performs much better than linear regression.
- Its performance on the independent test data well is even better than CV suggests. Could this be a consequence of the fact that certain diamonds appear multiple times in the data, introducing potential "leakage" from training to test data?

# Random Splitting?

The data is often *randomly split* into partitions or folds. As long as rows are *independent*, this leads to honest estimates of model performance as it ensures independent data partitions. 

When rows are not independent, e.g. with time series data or grouped data, such strategy is flawed and leads to too optimistic results. **This is one of the most frequent reasons to end up with a bad model. It is essential to avoid it.**

## Time-series data

When data represents a time series, splitting is best done in a way that does not destroy the temporal order. For simple validation, e.g. the first 80% of rows could be used for training and the remaining 20% for validation. 

## Grouped data

Often, data is grouped or clustered by some (hopefully known) ID variable, e.g.

- multiple rows belong to the same patient/customer or
- duplicated rows (accidental or not). 

Then, instead of distributing *rows* into partitions, we should distribute *groups*/IDs in order to not destroy the data structure and to get honest performance estimates. We speak of *grouped splitting* and *group k-fold CV*. 

In our example with diamonds data, it would be useful to have a column with diamond "id" that could be used for grouped splitting. (How would you create a proxy for this?)

## Stratification

*If rows are independent*, there is a variant of random splitting that often provides better results and is therefore frequently used: *stratified splitting*. With stratified splitting or *stratified k-fold CV*, rows are split to ensure approximately equal distribution of a key variable (the response or deciding covariable) across partitions/folds.

# Chapter Summary

In this chapter, we have met strategies to estimate model performance in a fair way. These strategies are also used for model selection and tuning. They are an essential part of the full modeling process. ML models without appropriate validation strategy are not to be trusted. 

# Exercises

1. Apply *stratified* splitting (on the response `price`) throughout the last example. Do the results change?
2. As an alternative to *grouped* splitting, repeat the last example also on data deduplicated by `price` and covariables. Do the results change? Why should we trust these results more than earlier results?
3. Use cross-validation to select the best polynomial degree to represent `log(carat)` in the Gamma GLM with log-link (with additional covariables `color`, `cut`, and `clarity`). Evaluate the result on an independent test data.
4. Optional: Compare the linear regression for `price` (using `log(carat)`, `color`, `cut`, and `clarity` as covariables) with a corresponding Gamma GLM with log-link by simple validation. Use once (R)MSE for comparison and once Gamma deviance. What do you observe?


