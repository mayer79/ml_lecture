---
title: "Solutions Chapter 3"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    theme: paper
    code_folding: show
    math_method: katex
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {rmarkdown::render(input, output_dir = "../docs")})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE, 
  message = FALSE
)
```

# Exercises on Random Forests

## Exercise 1

```{r}
library(tidyverse)
library(withr)
library(ranger)
library(MetricsWeighted)

# Train/test split
with_seed(
  9838,
  ix <- sample(nrow(diamonds), 0.8 * nrow(diamonds))
)

diamonds <- transform(diamonds, log_carat = log(carat))

fit <- ranger(
  price ~ log_carat + color + cut + clarity, 
  num.trees = 500,
  data = diamonds[ix, ], 
  importance = "impurity",
  seed = 83
)
fit

# Performance on test data
pred <- predict(fit, diamonds[-ix, ])$predictions
rmse(diamonds$price[-ix], pred)       # 553 USD
train_mean <- mean(diamonds[["price"]][ix])
r_squared(diamonds$price[-ix], pred, reference_mean = train_mean)  # 0.9814
```

**Comment:** The results are essentially identical because log is a monotonic transformation. Differences might come from implementation tricks.

## Exercise 2

```{r}
library(ggplot2)
library(withr)
library(ranger)
library(hstats)
library(MetricsWeighted)
library(insuranceData)

data(dataCar)

# Train/test split (stratified on response)
with_seed(
  9838,
  ix <- sample(nrow(dataCar), 0.8 * nrow(dataCar))
)

# Instead of systematic grid search, manually select good tree depth by OOB
fit <- ranger(
  clm ~ veh_value + veh_body + veh_age + gender + area + agecat,
  data = dataCar[ix, ], 
  probability = TRUE, 
  max.depth = 5,
  importance = "impurity",
  seed = 3938
)
fit # OOB prediction using Brier score (= MSE) 0.062984

pred <- predict(fit, dataCar[-ix, ])$predictions[, 2]
mse(dataCar[-ix, "clm"], pred)  # 0.0651
train_mean <- mean(dataCar[["clm"]][ix])
r_squared(dataCar[-ix, "clm"], pred, reference_mean = train_mean) # 0.0021

# Test performance with small tree depth seems to be best. 
# When studying relative performance metrics like the relative MSE gain, 
# we can see that performance of the model is very low. 
# TPL claims seem to be mostly determined by bad luck, which makes sense.

# Variable importance regarding Gini improvement
imp <- sort(importance(fit))
imp <- imp / sum(imp)
barplot(imp, horiz = TRUE, col = "orange", cex.names = 0.8, las = 2)

# Partial dependence plots (need prediction function to get only second column)
pred_fun <- function(m, X) predict(m, X)$predictions[, 2]

for (v in c("veh_value", "veh_body", "veh_age", "gender", "area", "agecat")) {
  p <- partial_dep(fit, v = v, X = dataCar[ix, ], pred_fun = pred_fun) |> 
    plot(rotate_x = (v == "veh_body")) +
    ggtitle(paste("PDP for", v)) 
  print(p)
}
```

**Comment:** Test performance with small tree depth seems to be best. When studying relative performance metrics like the relative deviance gain, we can see that performance of the model is very low. TPL claims seem to be mostly determined by bad luck, which makes sense.

# Exercises on Boosting

## Exercise 1

Just copy paste parts of the code in the lecture notes.

```{r}
library(ggplot2)
library(xgboost)
library(withr)
library(hstats)

y <- "price"
xvars <- c("carat", "color", "cut", "clarity")

# Split into train and test
with_seed(
  9838,
  ix <- sample(nrow(diamonds), 0.8 * nrow(diamonds))
)

y_train <- diamonds[[y]][ix]
X_train <- diamonds[ix, xvars]

y_test <- diamonds[[y]][-ix]
X_test <- diamonds[-ix, xvars]

# XGBoost data interface
dtrain <- xgb.DMatrix(data.matrix(X_train), label = y_train)

# Load grid and select best iteration
grid <- readRDS("gridsearch/diamonds_xgb.rds")
grid <- grid[order(grid$cv_score), ]
head(grid)

# Fit final, tuned model
params <- as.list(grid[1, -(1:3)])
params$monotone_constraints <- c(1, 0, 0, 0)

fit <- xgb.train(params = params, data = dtrain, nrounds = grid[1, "iteration"])

# Partial dependence plot for carat
partial_dep(fit, v = "carat", X = data.matrix(X_train)) |> 
  plot()
```

**Comment:** The argument is called "monotone_constraints". For each covariate, a value 0 means no constraint, a value -1 means a negative constraints, and a value 1 means positive constraint. Applying the constraint now leads to a monotonically increasing partial dependence plot. This is extremely useful in practice. Besides monotonic constraints, also interaction constraints are possible.

## Exercise 2

```{r}
# We adapt the template from the script
library(ggplot2)
library(xgboost)
library(withr)
library(insuranceData)
library(MetricsWeighted)
library(hstats)

data(dataCar)

xvars <- c("veh_value", "veh_body", "veh_age", "gender", "area", "agecat")
y <- "clm"

# Split into train and test
with_seed(
  9838,
  ix <- sample(nrow(dataCar), 0.8 * nrow(dataCar))
)

y_train <- dataCar[ix, y]
X_train <- dataCar[ix, xvars]

y_test <- dataCar[-ix, y]
X_test <- dataCar[-ix, xvars]

# XGBoost data handler
dtrain <- xgb.DMatrix(data.matrix(X_train), label = y_train)

# If grid search is to be run again, set tune <- TRUE
tune <- FALSE

if (tune) {
  # Use default parameters to set learning rate with suitable number of rounds
  params <- list(
    learning_rate = 0.02,
    objective = "binary:logistic"
  )
  
  # Cross-validation
  cvm <- xgb.cv(
    params = params,
    data = dtrain,
    nrounds = 5000,
    nfold = 5,
    early_stopping_rounds = 20,
    showsd = FALSE, 
    print_every_n = 50
  )
  cvm 
  
  # Grid
  grid <- expand.grid(
    iteration = NA,
    cv_score = NA,
    train_score = NA,
    objective = "binary:logistic",
    learning_rate = 0.02,
    max_depth = 3:6, 
    reg_lambda = c(0, 2.5, 5, 7.5),
    reg_alpha = c(0, 4),
    colsample_bynode = c(0.8, 1), 
    subsample = c(0.8, 1), 
    min_split_loss = c(0, 1e-04),
    min_child_weight = c(0.1, 1)
  )
  
  # Grid search or randomized search if grid is too large
  max_size <- 32
  grid_size <- nrow(grid)
  if (grid_size > max_size) {
    grid <- grid[sample(grid_size, max_size), ]
    grid_size <- max_size
  }
  
  # Loop over grid and fit XGBoost with five-fold CV and early stopping
  pb <- txtProgressBar(0, grid_size, style = 3)
  for (i in seq_len(grid_size)) {
    cvm <- xgb.cv(
      params = as.list(grid[i, -(1:2)]),
      data = dtrain,
      nrounds = 5000,
      nfold = 5,
      early_stopping_rounds = 20,
      verbose = 0
    )
    
    # Store result
    grid[i, 1] <- cvm$best_iteration
    grid[i, 2:3] <- cvm$evaluation_log[, c(4, 2)][cvm$best_iteration]
    setTxtProgressBar(pb, i)
    
    # Save grid to survive hard crashs. If interactive, use r/gridsearch/...
    saveRDS(grid, file = "gridsearch/claims_xgb.rds")
  }
}

# Load grid and select best iteration. If interactive use r/gridsearch/...
grid <- readRDS("gridsearch/claims_xgb.rds")
grid <- grid[order(grid$cv_score), ]
head(grid)

# Fit final, tuned model
fit <- xgb.train(
  params = as.list(grid[1, -(1:3)]), 
  data = dtrain, 
  nrounds = grid[1, "iteration"]
)

# Interpretation

# Performance on test data
pred <- predict(fit, data.matrix(X_test))
deviance_bernoulli(y_test, pred)

# Relative performance
r_squared_bernoulli(y_test, pred, reference_mean = mean(y_train))  # 0.0048
# Relative performance gain is very low, but better than of the tuned random forest.

# Variable importance regarding total loss improvement
imp <- xgb.importance(model = fit)
xgb.plot.importance(imp, col = "chartreuse4")

# PDPs
pred_fun <- function(m, x) predict(m, data.matrix(x))
for (v in xvars) {
  p <- partial_dep(fit, v = v, X = X_train, pred_fun = pred_fun) |> 
    plot(rotate_x = (v == "veh_body")) +
    ggtitle(paste("PDP for", v)) 
  print(p)
}
```

## Exercise 3 (Optional)

```{r}
# We slightly adapt the XGBoost template
library(ggplot2)
library(lightgbm)
library(withr)
library(insuranceData)
library(MetricsWeighted)
library(hstats)

data(dataCar)

xvars <- c("veh_value", "veh_body", "veh_age", "gender", "area", "agecat")
y <- "clm"

# Split into train and test
with_seed(
  9838,
  ix <- sample(nrow(dataCar), 0.8 * nrow(dataCar))
)

y_train <- dataCar[ix, y]
X_train <- dataCar[ix, xvars]

y_test <- dataCar[-ix, y]
X_test <- dataCar[-ix, xvars]

# We could even set categorical_feature = "veh_body" in order
# to treat that feature unordered categorical
dtrain <- lgb.Dataset(
  data.matrix(X_train), label = y_train, params = list(feature_pre_filter = FALSE)
)

# If grid search is to be run again, set tune <- TRUE
tune <- FALSE

if (tune) {
  # Use default parameters to set learning rate with suitable number of rounds
  params <- list(
    objective = "binary",
    learning_rate = 0.002
  )
  
  # Cross-validation
  cvm <- lgb.cv(
    params = params,
    data = dtrain,
    nrounds = 5000,
    nfold = 5,
    early_stopping_rounds = 20,
    eval_freq = 50
  )
  cvm 
  
  # Grid
  grid <- expand.grid(
    iteration = NA,
    score = NA,
    objective = "binary",
    learning_rate = 0.002,
    num_leaves = c(15, 31, 63),
    lambda_l2 = c(0, 2.5, 5, 7.5),
    lambda_l1 = c(0, 4),
    colsample_bynode = c(0.8, 1), 
    bagging_fraction = c(0.8, 1), 
    min_data_in_leaf = c(10, 20, 50), # unclear whether this works without calling lgb.Dataset()
    min_sum_hessian_in_leaf = c(0.001, 0.1),
    stringsAsFactors = FALSE
  )
  
  # Grid search or randomized search if grid is too large
  max_size <- 32
  grid_size <- nrow(grid)
  if (grid_size > max_size) {
    grid <- grid[sample(grid_size, max_size), ]
    grid_size <- max_size
  }
  
  # Loop over grid and fit LGB with five-fold CV and early stopping
  pb <- txtProgressBar(0, grid_size, style = 3)
  for (i in seq_len(grid_size)) {
    cvm <- lgb.cv(
      params = as.list(grid[i, -(1:2)]),
      data = dtrain,
      nrounds = 5000,
      nfold = 5,
      early_stopping_rounds = 20,
      verbose = -1
    )
    
    # Store result
    grid[i, 1:2] <- as.list(cvm)[c("best_iter", "best_score")]
    setTxtProgressBar(pb, i)
    
    # Save grid to survive hard crashs. If interactive, use r/gridsearch/...
    saveRDS(grid, file = "gridsearch/claims_lgb.rds")
  }
}

# Load grid and select best iteration. If interactive use r/gridsearch/...
grid <- readRDS("gridsearch/claims_lgb.rds")
grid <- grid[order(grid$score), ]
head(grid)

# Fit final, tuned model
fit <- lgb.train(
  params = as.list(grid[1, -(1:2)]), 
  data = dtrain, 
  
  nrounds = grid[1, "iteration"]
)

# Interpretation

# Performance on test data
pred <- predict(fit, data.matrix(X_test))
deviance_bernoulli(y_test, pred)

# Relative performance
r_squared_bernoulli(y_test, pred, reference_mean = mean(y_train)) # 0.0037
# Relative performance gain is a bit lower than with XGB

# Variable importance regarding total loss improvement
imp <- lgb.importance(model = fit)
lgb.plot.importance(imp)

# PDPs
pred_fun <- function(m, x) predict(m, data.matrix(x))
for (v in xvars) {
  p <- partial_dep(fit, v = v, X = X_train, pred_fun = pred_fun) |> 
    plot(rotate_x = (v == "veh_body")) +
    ggtitle(paste("PDP for", v)) 
  print(p)
}
```
