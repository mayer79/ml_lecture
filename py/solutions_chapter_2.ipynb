{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Model Selection and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 2 imports and data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, GammaRegressor\n",
    "from sklearn.preprocessing import (\n",
    "    OrdinalEncoder,\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    FunctionTransformer,\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "\n",
    "diamonds = pd.read_parquet(\"diamonds.parquet\")  # Or sns.load_dataset(\"diamonds\")\n",
    "\n",
    "ord_vars = [\"color\", \"cut\", \"clarity\"]\n",
    "ord_levels = [diamonds[x].cat.categories.to_list() for x in ord_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE DIFFERENCE TO THE CODE IN THE LECTURE NOTES\n",
    "dia = diamonds.drop_duplicates([\"price\", \"carat\"] + ord_vars)\n",
    "\n",
    "df_train, df_test, y_train, y_test = train_test_split(\n",
    "    dia, dia[\"price\"], test_size=0.2, random_state=49\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define knn model\n",
    "knn_preprocessor = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"ordered\", OrdinalEncoder(categories=ord_levels), ord_vars),\n",
    "            (\"linear\", \"passthrough\", [\"carat\"]),\n",
    "        ],\n",
    "        verbose_feature_names_out=False,\n",
    "    ),\n",
    "    StandardScaler(),\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "knn_model = Pipeline(\n",
    "    steps=[\n",
    "        (\"prep\", knn_preprocessor),\n",
    "        (\"knn\", KNeighborsRegressor(n_neighbors=5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define linear model\n",
    "linear_regression = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"log_carat\", FunctionTransformer(np.log), [\"carat\"]),\n",
    "            (\"dummies\", OneHotEncoder(categories=ord_levels, drop=\"first\"), ord_vars),\n",
    "        ]\n",
    "    ),\n",
    "    LinearRegression(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression CV RMSE: 1938.481\n"
     ]
    }
   ],
   "source": [
    "# CV performance of linear model\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=4432)\n",
    "\n",
    "result_linear = -cross_val_score(\n",
    "    linear_regression,\n",
    "    X=df_train,\n",
    "    y=y_train,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=cv,\n",
    ").mean()\n",
    "\n",
    "print(f\"Linear regression CV RMSE: {result_linear:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k of k-NN: 7\n",
      "Its CV-RMSE: 727.155\n"
     ]
    }
   ],
   "source": [
    "# CV performance of k-nearest-neighbour for k = 1-20\n",
    "search = GridSearchCV(\n",
    "    knn_model,\n",
    "    param_grid={\"knn__n_neighbors\": range(1, 21)},\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=cv,\n",
    ")\n",
    "\n",
    "# Remember: the best model is refitted on training data\n",
    "search.fit(X=df_train, y=y_train)\n",
    "best_k = search.best_params_[\"knn__n_neighbors\"]\n",
    "print(f\"Best k of k-NN: {best_k}\")\n",
    "print(f\"Its CV-RMSE: {-search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of final model: 700.575\n"
     ]
    }
   ],
   "source": [
    "# The overall best model seems to be 7-nearest-neighbour\n",
    "final_rmse = root_mean_squared_error(y_test, search.predict(df_test))\n",
    "print(f\"Test RMSE of final model: {final_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:** The test performance of the best model (7-NN) seems clearly worse than the one without deduplication (~700 USD RMSE vs ~620). Overall, this is probably the more realistic performance than the one obtained from the original data set. Still, as certain rows could be identical by chance, our deduplication approach might be slightly too conservative. The true performance will probably be somewhere between the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "df_train, df_test, y_train, y_test = train_test_split(\n",
    "    diamonds, diamonds[\"price\"], test_size=0.2, random_state=49\n",
    ")\n",
    "\n",
    "# Define parametrized end-to-end preprocessor with GLM on top of it\n",
    "prep_carat = Pipeline(\n",
    "    steps=[\n",
    "        (\"log\", FunctionTransformer(np.log)),\n",
    "        (\"scaler\", StandardScaler()),  # Not really necessary\n",
    "        (\"poly\", PolynomialFeatures(degree=1, include_bias=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"carat\", prep_carat, [\"carat\"]),\n",
    "        (\"dummies\", OneHotEncoder(categories=ord_levels, drop=\"first\"), ord_vars),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gamma_regression = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"glm\", GammaRegressor(alpha=0, solver=\"newton-cholesky\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ml_lecture\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"d:\\ml_lecture\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_gamma_deviance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.016315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.015877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.015921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.016914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_gamma_deviance\n",
       "1              0.018113\n",
       "2              0.017934\n",
       "3              0.016200\n",
       "4              0.016201\n",
       "5              0.016153\n",
       "6              0.016619\n",
       "7              0.016315\n",
       "8              0.015877\n",
       "9              0.015921\n",
       "10             0.016914"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search for best polynomial degree\n",
    "param_grid = {\"preprocessor__carat__poly__degree\": range(1, 11)}\n",
    "\n",
    "search = GridSearchCV(\n",
    "    gamma_regression,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_gamma_deviance\",\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=4302),\n",
    ")\n",
    "search.fit(X=df_train, y=y_train)\n",
    "\n",
    "# Organize results\n",
    "results = pd.DataFrame(\n",
    "    -search.cv_results_[\"mean_test_score\"],\n",
    "    index=param_grid[\"preprocessor__carat__poly__degree\"],\n",
    "    columns=[\"mean_gamma_deviance\"],\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean deviance on the test data: 0.016\n"
     ]
    }
   ],
   "source": [
    "# The model was automatically refitted on the train data with optimal degree:\n",
    "print(f\"Mean deviance on the test data: {-search.score(df_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:** The optimal degree seems to be 8 with a CV deviance of 0.016. The test performance is similar. Caution: Instead of using such high degree polynomial, it is better to use regression splines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (optional)\n",
    "\n",
    "Solution not shown here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ml_lecture')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da92bf8b1c4e537ce884cea261fe5226afea19e45e5e1a8d784507cf84f781ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
